{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run Multi-Step Kubeflow Pipeline from Notebook using Lightweight Components \n",
    "\n",
    "\n",
    "Lightweight Python components do not require you to build a new container image for every code change. Theyâ€™re intended for fast iteration in a notebook environment. This allows you to quikly build multi step pipeline from jupyter notebook and run on Kubeflow.\n",
    "\n",
    "**Advantages over container components**\n",
    "\n",
    "- Faster iteration: No need to build new container image after every change (building images takes some time).\n",
    "- Easier authoring: Components can be created in a local environment. Docker and Kubernetes are not required.\n",
    "\n",
    "[Read more on requirements to build lightwieght components](https://www.kubeflow.org/docs/pipelines/sdk/lightweight-python-components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "#!pip install kfp\n",
    "#!pip install python-dotenv\n",
    "#!pip install adal --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(train_datapath) -> str:\n",
    "    print(train_datapath)    \n",
    "    processed_data_path= train_datapath + '/processed'\n",
    "    return processed_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advanced function\n",
    "#Demonstrates imports, helper functions and multiple outputs\n",
    "from typing import NamedTuple\n",
    "def train_model(train_data_path:str, epoch : int) -> NamedTuple('TrainModelOutput', [('model_path', str), ('mlpipeline_ui_metadata', 'UI_metadata'), ('mlpipeline_metrics', 'Metrics')]):\n",
    "    '''Divides two numbers and calculate  the quotient and remainder'''\n",
    "    #Pip installs inside a component function.\n",
    "    #NOTE: installs should be placed right at the beginning to avoid upgrading a package\n",
    "    # after it has already been imported and cached by python\n",
    "    # If possible avoid installs and include them in base image \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    \n",
    "    #Imports inside a component function:\n",
    "    import sklearn\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "\n",
    "    #This function demonstrates how to use nested functions inside a component function:\n",
    "    def divmod_helper(dividend, divisor):\n",
    "        return np.divmod(dividend, divisor)\n",
    "\n",
    "    #Train model placeholder\n",
    "    model_path= train_data_path + \"/trained_model/model.pkl\" \n",
    "    print(model_path)\n",
    "    \n",
    "    # Exports a sample tensorboard:\n",
    "    metadata = {\n",
    "      'outputs' : [{\n",
    "        'type': 'model',\n",
    "        'source': '/path/to/model',\n",
    "      }]\n",
    "    }\n",
    "    with open('/mlpipeline_ui_metadata.json', 'w') as f:\n",
    "       json.dump(metadata, f)\n",
    "\n",
    "\n",
    "    train_loss = '11'\n",
    "    metrics = {\n",
    "        'metrics': [{\n",
    "          'name': 'loss', # The name of the metric. Visualized as the column name in the runs table.\n",
    "          'numberValue':  train_loss, # The value of the metric. Must be a numeric value.\n",
    "          'format': \"PERCENTAGE\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "        }]\n",
    "      }\n",
    "    with open('/mlpipeline-metrics.json', 'w') as f:\n",
    "       json.dump(metrics, f)\n",
    "\n",
    "\n",
    "    TrainModelOutput = namedtuple('TrainModelOutput', ['model_path', 'mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return TrainModelOutput(model_path,json.dumps(metadata), json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "def evaluate_model(trained_model_path:str) -> NamedTuple('EvaluateModelOutput', [('mlpipeline_metrics', 'Metrics')]):\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "    print(trained_model_path)    \n",
    "    #Evalaute model placeholder\n",
    "    \n",
    "    accuracy_score = '70'\n",
    "    metrics = {\n",
    "        'metrics': [{\n",
    "          'name': 'accuracy_score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "          'numberValue':  accuracy_score, # The value of the metric. Must be a numeric value.\n",
    "          'format': \"PERCENTAGE\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "        }]\n",
    "      }\n",
    "    with open('/mlpipeline-metrics.json', 'w') as f:\n",
    "       json.dump(metrics, f)\n",
    "    \n",
    "    EvaluteModelOutput = namedtuple('EvaluteModelOutput', ['mlpipeline_metrics'])\n",
    "    return EvaluteModelOutput(json.dumps(metrics))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and compile Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.compiler as compiler\n",
    "@dsl.pipeline(\n",
    "   name='Local Dev pipeline',\n",
    "   description='Pipeline demonstrating local dev experiece with Kubeflow'\n",
    ")\n",
    "def tacosandburritos_train():\n",
    "    process_data_op = comp.func_to_container_op(process_data,base_image='kubeflowyoacr.azurecr.io/mexicanfood/notebook-comp:latest')\n",
    "    process_data_task = process_data_op(\"/train_data\") #Returns a dsl.ContainerOp class instance. \n",
    "    \n",
    "    train_model_op = comp.func_to_container_op(train_model,base_image='kubeflowyoacr.azurecr.io/mexicanfood/notebook-comp:latest')\n",
    "    train_model_task = train_model_op(process_data_task.output,5) #Returns a dsl.ContainerOp class instance. \n",
    "    \n",
    "    evaluate_model_op = comp.func_to_container_op(evaluate_model,base_image='kubeflowyoacr.azurecr.io/mexicanfood/notebook-comp:latest')\n",
    "    evaluate_model_task = evaluate_model_op(train_model_task.output) #Returns a dsl.ContainerOp class instance. \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    compiler.Compiler().compile(tacosandburritos_train, 'pipeline.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Verify compiled pipeline in directory\n",
    "ls *.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environement Variables from .ENV\n",
    "\n",
    "**Note**: Make sure the .env file is in the same directory as this notebook.Do not check .env with secerets values in your repo. Refere the env.example from the repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load env variable from\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload and run pipeline on kubeflow \n",
    "\n",
    "Upload compiled pipeline to Kubeflow, create experiment and run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kubernetes import client as k8s_client\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as components\n",
    "from kfp.azure import use_azure_secret\n",
    "import uuid\n",
    "import subprocess\n",
    "import adal\n",
    "\n",
    "authorityHostUrl = \"https://login.microsoftonline.com\"\n",
    "GRAPH_RESOURCE = '00000002-0000-0000-c000-000000000000'\n",
    "authority_url = authorityHostUrl + '/' + os.environ.get(\"TENANT_ID\")\n",
    "context = adal.AuthenticationContext(authority_url)\n",
    "token = context.acquire_token_with_client_credentials(GRAPH_RESOURCE, os.environ.get(\"SP_APP_ID\"), os.environ.get(\"SP_APP_SECRET\"))  # noqa: E501\n",
    "client = kfp.Client(os.environ.get(\"KFP_HOST\"), existing_token=token['accessToken'])\n",
    "pipeline_file = os.path.join('pipeline.tar.gz')\n",
    "pipeline_name = os.environ.get(\"PIPELINE_NAME\") + \"-\" + str(uuid.uuid4())\n",
    "#Upload Pipeline \n",
    "pipeline = client.pipeline_uploads.upload_pipeline(pipeline_file, name=pipeline_name)   \n",
    "exp = client.get_experiment(experiment_name=os.environ.get(\"EXP_NAME\"))\n",
    "# Create parameters and passto run_pipeline if your pipeline takes paprameters \n",
    "# pipeline_params = {}\n",
    "# pipeline_params[\"paramname\"] = paramvalue\n",
    "# Run pipeline\n",
    "client.run_pipeline(exp.id, job_name=os.environ.get(\"RUN_NAME\"),pipeline_id=pipeline.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit6a44e28f3a7c4ee6aa37ab5d8cd3c5ba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}